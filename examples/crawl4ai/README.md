# Crawl4AI Integration Examples

This directory contains comprehensive examples demonstrating how to integrate Crawl4AI into your applications with various architectures and use cases.

## ğŸ“ Project Structure

```
examples/crawl4ai/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .gitignore                  # Git ignore rules
â”œâ”€â”€ basic_example.py            # Simple crawling example
â”œâ”€â”€ batch_example.py            # Batch processing example
â”œâ”€â”€ advanced_example.py         # Advanced features runner
â”œâ”€â”€ fastapi_example.py          # FastAPI integration
â”œâ”€â”€ advanced_examples/          # Modular advanced examples
â”‚   â”œâ”€â”€ browser_config.py       # Browser configuration
â”‚   â”œâ”€â”€ extraction_strategies.py # Content extraction
â”‚   â””â”€â”€ cache_and_performance.py # Performance optimization
â”œâ”€â”€ fastapi_integration/        # Modular FastAPI components
â”‚   â”œâ”€â”€ models.py              # Pydantic models
â”‚   â”œâ”€â”€ crawler_service.py     # Crawler management
â”‚   â”œâ”€â”€ routes.py              # API endpoints
â”‚   â””â”€â”€ background_tasks.py    # Async task handling
â””â”€â”€ react-crawl4ai-demo/       # React frontend
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ components/
    â”‚   â”‚   â”œâ”€â”€ CrawlForm.jsx   # Crawling form
    â”‚   â”‚   â””â”€â”€ CrawlResults.jsx # Results display
    â”‚   â””â”€â”€ App.jsx             # Main application
    â””â”€â”€ package.json
```

## ğŸš€ Examples Overview

### 1. Basic Example (`basic_example.py`)
Simple introduction to Crawl4AI with essential features:
- Basic web crawling setup
- Content extraction and display
- Screenshot and PDF generation
- Error handling and logging

### 2. Batch Processing (`batch_example.py`)
Efficient processing of multiple URLs:
- Concurrent crawling with rate limiting
- Resource monitoring and management
- Progress tracking and reporting
- Performance comparison strategies

### 3. Advanced Examples (`advanced_example.py`)
Comprehensive feature demonstration split into modules:
- **Browser Configuration**: Custom user agents, viewports, JavaScript execution
- **Extraction Strategies**: CSS selectors, XPath, LLM-powered extraction
- **Cache & Performance**: Cache modes, optimization techniques, memory monitoring

### 4. FastAPI Integration (`fastapi_example.py`)
Production-ready API service with modular architecture:
- RESTful API endpoints with validation
- Async request handling and background tasks
- Comprehensive error handling and logging
- Interactive API documentation

### 5. React Frontend (`react-crawl4ai-demo/`)
Modern web interface for crawling operations:
- Interactive form with all crawling options
- Real-time results display and analysis
- Content viewing, copying, and downloading
- Responsive design with Tailwind CSS

## ğŸ› ï¸ Quick Start

### Prerequisites
```bash
# Install Python dependencies
pip install -r requirements.txt

# For React frontend (optional)
cd react-crawl4ai-demo
npm install  # or pnpm install
```

### Running Examples

#### 1. Basic Crawling
```bash
python basic_example.py
```

#### 2. Batch Processing
```bash
python batch_example.py
```

#### 3. Advanced Features
```bash
# Run all advanced examples
python advanced_example.py

# Run specific modules
python advanced_example.py browser_config
python advanced_example.py extraction
python advanced_example.py performance
```

#### 4. FastAPI Backend
```bash
# Start the API server
python fastapi_example.py

# Visit API documentation
open http://localhost:8000/docs
```

#### 5. React Frontend
```bash
# Start development server
cd react-crawl4ai-demo
npm run dev

# Visit the application
open http://localhost:5173
```

## ğŸ¯ Target URLs

All examples use GitHub-related URLs for demonstration:
- Primary: `https://github.com/CaptainASIC`
- Additional: Profile tabs, repositories, etc.

## ğŸ“Š Features Demonstrated

### Core Crawling Features
- âœ… Basic web page crawling
- âœ… Content extraction (HTML, Markdown)
- âœ… Screenshot capture
- âœ… PDF generation
- âœ… Media extraction
- âœ… Link analysis

### Advanced Configuration
- âœ… Custom browser settings
- âœ… User agent customization
- âœ… Viewport configuration
- âœ… JavaScript execution
- âœ… Cookie management
- âœ… Header customization

### Extraction Strategies
- âœ… CSS selector-based extraction
- âœ… XPath-based extraction
- âœ… LLM-powered extraction (with OpenAI)
- âœ… Custom extraction schemas
- âœ… Structured data extraction

### Performance & Optimization
- âœ… Cache management (multiple modes)
- âœ… Batch processing with concurrency control
- âœ… Rate limiting and retry logic
- âœ… Memory usage monitoring
- âœ… Performance benchmarking

### Integration Patterns
- âœ… FastAPI REST API
- âœ… React frontend integration
- âœ… Background task processing
- âœ… Real-time progress tracking
- âœ… Error handling and recovery

## ğŸ”§ Configuration Options

### Browser Configuration
```python
browser_config = BrowserConfig(
    browser_type="chromium",
    headless=True,
    user_agent="Custom-Agent/1.0",
    viewport_width=1920,
    viewport_height=1080,
    java_script_enabled=True
)
```

### Crawling Configuration
```python
run_config = CrawlerRunConfig(
    css_selector="main, article",
    word_count_threshold=10,
    screenshot=True,
    pdf=True,
    extract_media=True,
    cache_mode=CacheMode.ENABLED
)
```

## ğŸŒ API Endpoints (FastAPI)

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/` | API information |
| GET | `/health` | Health check |
| POST | `/crawl` | Single URL crawling |
| POST | `/batch-crawl` | Multiple URL crawling |
| POST | `/extract` | Structured content extraction |
| POST | `/crawl-async` | Asynchronous crawling |
| GET | `/status/{task_id}` | Task status |
| GET | `/tasks` | List all tasks |

## ğŸ“± Frontend Features

- **Interactive Form**: Configure all crawling parameters
- **Real-time Results**: View extracted content immediately
- **Content Analysis**: Statistics, links, and media breakdown
- **Export Options**: Copy or download results
- **Responsive Design**: Works on desktop and mobile
- **Error Handling**: Graceful error display and recovery

## ğŸ” Example Use Cases

1. **Content Migration**: Extract content from old websites
2. **SEO Analysis**: Analyze page structure and links
3. **Data Collection**: Gather structured data from web pages
4. **Monitoring**: Track changes in web content
5. **Research**: Collect information from multiple sources
6. **Testing**: Validate web page rendering and content

## ğŸš¨ Important Notes

- **API Keys**: Set `OPENAI_API_KEY` environment variable for LLM extraction
- **Rate Limiting**: Be respectful of target websites
- **Error Handling**: All examples include comprehensive error handling
- **Performance**: Monitor resource usage for large-scale operations
- **Security**: Validate all inputs in production environments

## ğŸ“š Additional Resources

- [Crawl4AI Documentation](https://docs.crawl4ai.com/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [React Documentation](https://react.dev/)
- [GitHub Repository](https://github.com/CaptainASIC)

## ğŸ¤ Contributing

Feel free to extend these examples with additional features or improvements. Each module is designed to be self-contained and easily modifiable.

